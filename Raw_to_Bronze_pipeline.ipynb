{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b165e62-5c2e-4521-b187-8d1accd67048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RawTables to Bronze Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ff47ca-656e-49c8-a7d8-e4a2eb7bf609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"chicago_inspections_bronze\",\n",
    "    comment=\"Chicago food inspection data from raw table with deduplication - Streaming Table\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"delta.enableChangeDataFeed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def chicago_bronze():\n",
    "    \"\"\"Ingest Chicago inspection data from raw table with deduplication\"\"\"\n",
    "    \n",
    "    # Read from raw table as stream\n",
    "    chicago_df = (\n",
    "        spark.readStream\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .table(\"food_inspection.raw.chicago_inspections\")\n",
    "    )\n",
    "    \n",
    "    # Add ingestion metadata\n",
    "    chicago_with_metadata = chicago_df \\\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"_source_file\", lit(\"raw_table\"))\n",
    "    \n",
    "    # Drop any reserved CDF columns if they exist to ensure clean processing\n",
    "    cdf_reserved_columns = [\"_change_type\", \"_commit_version\", \"_commit_timestamp\"]\n",
    "    for col_name in cdf_reserved_columns:\n",
    "        if col_name in chicago_with_metadata.columns:\n",
    "            chicago_with_metadata = chicago_with_metadata.drop(col_name)\n",
    "    \n",
    "    # Create hash for deduplication (exclude metadata columns)\n",
    "    chicago_columns = [c for c in chicago_with_metadata.columns if not c.startswith(\"_\")]\n",
    "    chicago_with_hash = chicago_with_metadata.withColumn(\n",
    "        \"row_hash\",\n",
    "        sha2(concat_ws(\"||\", *[coalesce(col(c), lit(\"NULL\")) for c in chicago_columns]), 256)\n",
    "    )\n",
    "    \n",
    "    # Deduplicate using watermark for streaming and drop the hash column\n",
    "    chicago_deduped = chicago_with_hash \\\n",
    "        .withWatermark(\"_ingestion_timestamp\", \"1 hour\") \\\n",
    "        .dropDuplicates([\"row_hash\"]) \\\n",
    "        .drop(\"row_hash\")\n",
    "    \n",
    "    return chicago_deduped\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dallas_inspections_bronze\",\n",
    "    comment=\"Dallas food inspection data from raw table with deduplication - Streaming Table\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"delta.enableChangeDataFeed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def dallas_bronze():\n",
    "    \"\"\"Ingest Dallas inspection data from raw table with deduplication\"\"\"\n",
    "    \n",
    "    # Read from raw table as stream\n",
    "    dallas_df = (\n",
    "        spark.readStream\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .table(\"food_inspection.raw.dallas_inspections\")\n",
    "    )\n",
    "    \n",
    "    # Add ingestion metadata\n",
    "    dallas_with_metadata = dallas_df \\\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"_source_file\", lit(\"raw_table\"))\n",
    "    \n",
    "    # Drop any reserved CDF columns if they exist to ensure clean processing\n",
    "    cdf_reserved_columns = [\"_change_type\", \"_commit_version\", \"_commit_timestamp\"]\n",
    "    for col_name in cdf_reserved_columns:\n",
    "        if col_name in dallas_with_metadata.columns:\n",
    "            dallas_with_metadata = dallas_with_metadata.drop(col_name)\n",
    "    \n",
    "    # Create hash for deduplication (exclude metadata columns)\n",
    "    dallas_columns = [c for c in dallas_with_metadata.columns if not c.startswith(\"_\")]\n",
    "    dallas_with_hash = dallas_with_metadata.withColumn(\n",
    "        \"row_hash\",\n",
    "        sha2(concat_ws(\"||\", *[coalesce(col(c), lit(\"NULL\")) for c in dallas_columns]), 256)\n",
    "    )\n",
    "    \n",
    "    # Deduplicate using watermark for streaming and drop the hash column\n",
    "    dallas_deduped = dallas_with_hash \\\n",
    "        .withWatermark(\"_ingestion_timestamp\", \"1 hour\") \\\n",
    "        .dropDuplicates([\"row_hash\"]) \\\n",
    "        .drop(\"row_hash\")\n",
    "    \n",
    "    return dallas_deduped"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Raw_to_Bronze_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
